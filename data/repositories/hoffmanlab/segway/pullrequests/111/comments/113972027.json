{"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/111/comments/113972027.json"}, "code": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/diff/rcwchan/segway-1:acff36c6771e..9d6cdcaa85b3?path=segway%2Frun.py"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/111/_/diff#comment-113972027"}}, "parent": {"id": 113970382, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/111/comments/113970382.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/111/_/diff#comment-113970382"}}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 111, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/111.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/111"}}, "title": "Modify Segway's scheduling to allow for 'batch' job submission in training"}, "content": {"raw": "1. Even if each batch script is unique to it\u2019s instance, the submitted job will be on a different host potentially reading from the file. The issue is not a write-write race condition but a write-read race condition that is exacerbated by the fact that there is a bunch of append operations taking place which may or may not sync by the same time another machine is reading from it.\n2. So yes it\u2019s a typical syncing on NFS issue. However if you just do a single write \\(with all jobs\\) it is much preferred since either you get all or nothing on another host anyway.\n\nGenerally if this all takes place on the same host it should not be a big issue I\u2019d imagine since NFS typically caches the writes out to files like crazy but then you\u2019d have to do all this weird job management inside segway-task. I\u2019m not sure what the best quick approach to this is. Perhaps partitioning jobs inside the RestartableJobDict when things are queued in and only writing out the would-be job when the limit for a partition is hit?\n\n@{557058:a9657985-692c-405c-995b-4e41cda7ba2b}  might have some insight on this issue.", "markup": "markdown", "html": "<ol>\n<li>Even if each batch script is unique to it\u2019s instance, the submitted job will be on a different host potentially reading from the file. The issue is not a write-write race condition but a write-read race condition that is exacerbated by the fact that there is a bunch of append operations taking place which may or may not sync by the same time another machine is reading from it.</li>\n<li>So yes it\u2019s a typical syncing on NFS issue. However if you just do a single write (with all jobs) it is much preferred since either you get all or nothing on another host anyway.</li>\n</ol>\n<p>Generally if this all takes place on the same host it should not be a big issue I\u2019d imagine since NFS typically caches the writes out to files like crazy but then you\u2019d have to do all this weird job management inside segway-task. I\u2019m not sure what the best quick approach to this is. Perhaps partitioning jobs inside the RestartableJobDict when things are queued in and only writing out the would-be job when the limit for a partition is hit?</p>\n<p><span class=\"ap-mention\" data-atlassian-id=\"557058:a9657985-692c-405c-995b-4e41cda7ba2b\">@Michael Hoffman</span>  might have some insight on this issue.</p>", "type": "rendered"}, "created_on": "2019-08-21T20:22:01.004051+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {"to": 268, "from": null, "outdated": true, "path": "segway/run.py"}, "updated_on": "2019-08-21T20:22:17.985525+00:00", "type": "pullrequest_comment", "id": 113972027}