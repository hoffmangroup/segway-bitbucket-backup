{"pagelen": 50, "values": [{"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97987928.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97987928"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "`num_segs` is referring to the number of labels? I had to read down to `for label in range(num_segs)` to get a clue. Perhaps something like `num_labels` would help.", "markup": "markdown", "html": "<p><code>num_segs</code> is referring to the number of labels? I had to read down to <code>for label in range(num_segs)</code> to get a clue. Perhaps something like <code>num_labels</code> would help.</p>", "type": "rendered"}, "created_on": "2019-04-08T16:48:05.653052+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {}, "updated_on": "2019-04-08T16:48:05.664971+00:00", "type": "pullrequest_comment", "id": 97987928}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97987081.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97987081"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "The only issue I have with this is for higher resolutions, say 200 bp like ChromHMM, a single defined prior in that region will be considered significant across that entire 200 bp region. I\u2019m not sure what a better heuristic is necessarily. Perhaps only consider any priors if they are defined for at least half of the downsampled region?", "markup": "markdown", "html": "<p>The only issue I have with this is for higher resolutions, say 200 bp like ChromHMM, a single defined prior in that region will be considered significant across that entire 200 bp region. I\u2019m not sure what a better heuristic is necessarily. Perhaps only consider any priors if they are defined for at least half of the downsampled region?</p>", "type": "rendered"}, "created_on": "2019-04-08T16:41:24.142405+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {}, "updated_on": "2019-04-08T16:41:24.152562+00:00", "type": "pullrequest_comment", "id": 97987081}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97986591.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97986591"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "add \u201cwith 3 labels\u201d since I was trying to math this out myself.", "markup": "markdown", "html": "<p>add \u201cwith 3 labels\u201d since I was trying to math this out myself.</p>", "type": "rendered"}, "created_on": "2019-04-08T16:37:29.022991+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {}, "updated_on": "2019-04-08T16:37:29.034848+00:00", "type": "pullrequest_comment", "id": 97986591}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97985580.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97985580"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "Isn\u2019t this conditional already covered by the CPP macros inside the input.master.tmpl?\n\nYou could probably keep this code as-is and not worry about having to define `VIRTUAL_EVIDENCE` as 1 in `segway.inc.tmpl` and avoid all that code overhead. As in it might be worth while letting Segway manage inserting virtual evidence or not based on `self.virtual_evidence` than using macros. I'm not sure what is exactly needed by the `VIRTUAL_EVIDENCE` macro.", "markup": "markdown", "html": "<p>Isn\u2019t this conditional already covered by the CPP macros inside the input.master.tmpl?</p>\n<p>You could probably keep this code as-is and not worry about having to define <code>VIRTUAL_EVIDENCE</code> as 1 in <code>segway.inc.tmpl</code> and avoid all that code overhead. As in it might be worth while letting Segway manage inserting virtual evidence or not based on <code>self.virtual_evidence</code> than using macros. I'm not sure what is exactly needed by the <code>VIRTUAL_EVIDENCE</code> macro.</p>", "type": "rendered"}, "created_on": "2019-04-08T16:29:28.863778+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {}, "updated_on": "2019-04-08T16:30:43.280583+00:00", "type": "pullrequest_comment", "id": 97985580}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97984126.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97984126"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "Where is this file used?", "markup": "markdown", "html": "<p>Where is this file used?</p>", "type": "rendered"}, "created_on": "2019-04-08T16:18:05.558667+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {}, "updated_on": "2019-04-08T16:18:05.577785+00:00", "type": "pullrequest_comment", "id": 97984126}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97983443.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97983443"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "Probably best to define FALSE as 0 and TRUE as anything else since `VIRTUAL_EVIDENCE` is just a flag in this case. This is pretty minor. If it doesn't make a difference though I would suggest chaning to `!= 0`", "markup": "markdown", "html": "<p>Probably best to define FALSE as 0 and TRUE as anything else since <code>VIRTUAL_EVIDENCE</code> is just a flag in this case. This is pretty minor. If it doesn't make a difference though I would suggest chaning to <code>!= 0</code></p>", "type": "rendered"}, "created_on": "2019-04-08T16:12:55.196719+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {}, "updated_on": "2019-04-08T16:12:55.212008+00:00", "type": "pullrequest_comment", "id": 97983443}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97983082.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97983082"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "These comments should be above the `VIRUTAL_EVIDENCE_LIST_FILENAME` since they are both relating to the GMTK cppCommandOption", "markup": "markdown", "html": "<p>These comments should be above the <code>VIRUTAL_EVIDENCE_LIST_FILENAME</code> since they are both relating to the GMTK cppCommandOption</p>", "type": "rendered"}, "created_on": "2019-04-08T16:10:16.424558+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {}, "updated_on": "2019-04-08T16:10:16.440687+00:00", "type": "pullrequest_comment", "id": 97983082}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97982106.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97982106"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "I\u2019m having a hard time following the difference between \u201cdelimeter for the prior string spec between different label:prior pairs\u201d and \u201cdeliminiter for each label:prior pair\u201d.\n\nPresumably it's something like `0:0.5,1:0.2`. Maybe a simple example would be easier to follow in the comments\n\nPerhaps Label delimiter vs Prior delimiter? Also I assume that you cannot do multiple assignment like `0,1,2:0.2` correct?", "markup": "markdown", "html": "<p>I\u2019m having a hard time following the difference between \u201cdelimeter for the prior string spec between different label:prior pairs\u201d and \u201cdeliminiter for each label:prior pair\u201d.</p>\n<p>Presumably it's something like <code>0:0.5,1:0.2</code>. Maybe a simple example would be easier to follow in the comments</p>\n<p>Perhaps Label delimiter vs Prior delimiter? Also I assume that you cannot do multiple assignment like <code>0,1,2:0.2</code> correct?</p>", "type": "rendered"}, "created_on": "2019-04-08T16:03:19.490924+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "inline": {}, "updated_on": "2019-04-08T16:04:43.340842+00:00", "type": "pullrequest_comment", "id": 97982106}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"comment": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98/comments/97981166.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98/_/diff#comment-97981166"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}, "content": {"raw": "Please update NEWS\n\nAdditionally after this review is complete, the tests will need to be updated and passing on your fork before merging", "markup": "markdown", "html": "<p>Please update NEWS</p>\n<p>Additionally after this review is complete, the tests will need to be updated and passing on your fork before merging</p>", "type": "rendered"}, "created_on": "2019-04-08T15:57:23.804399+00:00", "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}, "updated_on": "2019-04-08T15:57:23.817189+00:00", "type": "pullrequest_comment", "id": 97981166}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence \\(as there is a different Ct|Qt CPT at each t.\\) \r\n\r\n![](https://bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png)\r\n## User changes\r\n\r\nThere are no changes to users without VE.\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "0ec5ee955fd5", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/0ec5ee955fd5"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/0ec5ee955fd5"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-05T22:40:01.179965+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence \\(as there is a different Ct|Qt CPT at each t.\\) \r\n\r\n![](https://bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png)\r\n## User changes\r\n\r\nThere are no changes to users without VE.\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "cb46019e9bbc", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/cb46019e9bbc"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/cb46019e9bbc"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-05T14:41:10.134910+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence \\(as there is a different Ct|Qt CPT at each t.\\) \r\n\r\n![](https://bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png)\r\n## User changes\r\n\r\nThere are no changes to users without VE.\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "d2804f76aadd", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/d2804f76aadd"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/d2804f76aadd"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-05T00:13:33.792227+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence \\(as there is a different Ct|Qt CPT at each t.\\) \r\n\r\n![](https://bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png)\r\n## User changes\r\n\r\nThere are no changes to users without VE.\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "5fc149649b21", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/5fc149649b21"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/5fc149649b21"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T21:01:13.982223+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct.\r\n\r\n![](https://bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png)\r\n## User changes\r\n\r\nThere are no changes to users without VE.\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "5fc149649b21", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/5fc149649b21"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/5fc149649b21"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T20:59:14.486335+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model looks like this:\r\n\r\nTBD\r\n\r\n## User changes\r\n\r\nThere are no changes to users without VE.\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "5fc149649b21", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/5fc149649b21"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/5fc149649b21"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T20:41:50.840837+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model looks like this:\r\n\r\nTBD\r\n\r\n## User changes\r\n\r\nThere are no changes to users without VE--the VE node will simply have 0 presence \\(and thus 0 effect\\).\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "5fc149649b21", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/5fc149649b21"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/5fc149649b21"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T20:40:04.426473+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model looks like this:\r\n\r\nTBD\r\n\r\n## User changes\r\n\r\nThere are no changes to users without VE--the VE node will simply have 0 presence \\(and thus 0 effect\\).\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "18ed0af1af6b", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/18ed0af1af6b"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/18ed0af1af6b"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T16:20:25.799242+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model looks like this:\r\n\r\nTBD\r\n\r\n## User changes\r\n\r\nThere are no changes to users without VE--the VE node will simply have 0 presence \\(and thus 0 effect\\).\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed and distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "e6dc8a0adad4", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/e6dc8a0adad4"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/e6dc8a0adad4"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T16:07:39.798726+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model looks like this:\r\n\r\nTBD\r\n\r\n## User changes\r\n\r\nThere are no changes to users without VE--the VE node will simply have 0 presence \\(and thus 0 effect\\).\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel           0   1   2\r\nchr1    0   1   0.5 0.2 0.3\r\nchr1    1   2   0.3 0.4 0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1    0   10  0.4 0.3 0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\) in a future PR.", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "e6dc8a0adad4", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/e6dc8a0adad4"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/e6dc8a0adad4"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T15:51:13.133090+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model looks like this:\r\n\r\nTBD\r\n\r\n## User changes\r\n\r\nThere are no changes to users without VE--the VE node will simply have 0 presence \\(and thus 0 effect\\).\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel\t\t\t0\t1\t2\r\nchr1\t0\t1\t0.5\t0.2\t0.3\r\nchr1\t1\t2\t0.3\t0.4\t0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1\t0\t10\t0.4\t0.3\t0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\).", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "e6dc8a0adad4", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/e6dc8a0adad4"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/e6dc8a0adad4"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T15:48:33.134718+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}, {"update": {"description": "## Introduction\r\n\r\nThis pull request implements virtual evidence during training \\(disabled during identification\\). It is partly \\(namely the VE structure\\) based off of Max Libbrecht's measure-prop branch.  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model looks like this:\r\n\r\nTBD\r\n\r\n## User changes\r\n\r\nThere are no changes to users without VE--the VE node will simply have 0 presence \\(and thus 0 effect\\).\r\n\r\nIf a user wishes to use virtual evidence, they should supply a 'virtual evidence file' to Segway with the option `--virtual-evidence`. This virtual evidence file should be of BED3 format where the 4th column is a string containing the label:prior pairs the user wishes to specify, delimited by commas. For example,\r\n\r\n```\r\nchr1    0    1000    0:0.9,1:0.05\r\n```\r\n\r\nSpecifies a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n```\r\nchr1    0    1000    0:0.4\r\n```\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\nFor positions at which no prior is specified for all labels, a uniform probability is computed distributed amongst the labels \\(though given 0 weight by Segway later due to the presence variable\\).\r\n\r\n## How it works\r\n\r\nVirtual evidence is similar to supervision in that values need to be supplied at each GMTK frame \\(with multiple frames per Segway window\\). However, the values supplied are CPTs, not observations, and so it does not work \\(as far as I know\\) to place them in the usual int/float observation files.\r\n\r\nTo this end, I used Segway's existing temporary observation files framework to create temporary CPTs \\(one large CPT per window, with one vector of priors for each GMTK frame in that window\\) and a list of these CPTs \\(`virtual_evidence_list_filename`\\).\r\n\r\nDuring training, the cpp directive `VIRTUAL_EVIDENCE_LIST_FILENAME` is given a value of `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` when creating the task to send to segway-task. `VIRTUAL_EVIDENCE_LIST_FILENAME_PLACEHOLDER` is then replaced within the cppCommand string with the temporarily created `virtual_evidence_list_filename` \\(with its list of CPTs per window\\).\r\n\r\nThe presence for virtual evidence is supplied identically as for data and supervision labels--appended to the int block and subsequently passed forwards to GMTK for use.\r\n\r\n## Resolution and downsampling\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n```\r\nchr1    0   1    0:0.5,1:0.2\r\nchr1    1    2    1:0.4\r\n```\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n```\r\nlabel\t\t\t0\t1\t2\r\nchr1\t0\t1\t0.5\t0.2\t0.3\r\nchr1\t1\t2\t0.3\t0.4\t0.3\r\n```\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n```\r\nchr1\t0\t10\t0.4\t0.3\t0.4\r\n```\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nFollowing discussion with Eric, I created the following virtual evidence bedfile and supplied it to simpleseg:\r\n\r\n```\r\nchr1    0   1000    0:0.9\r\nchr1    4000    5000    0:0.9\r\n```\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n```\r\nchr1    0   1000    0   1000    .   0   1000    27,158,119\r\nchr1    1000    2000    1   1000    .   1000    2000    217,95,2\r\nchr1    2000    3000    2   1000    .   2000    3000    117,112,179\r\nchr1    3000    4000    3   1000    .   3000    4000    231,41,138\r\nchr1    4000    5000    0   1000    .   4000    5000    27,158,119\r\nchr1    5000    6000    1   1000    .   5000    6000    217,95,2\r\nchr1    6000    7000    2   1000    .   6000    7000    117,112,179\r\nchr1    7000    8000    3   1000    .   7000    8000    231,41,138\r\n```\r\n\r\nI have named this test case `simplevirtualevidence`.\r\n\r\n## Remaining work\r\n\r\nAs discussed with Max Libbrecht over email, it might be interesting to enable VE during identification \\(allowing priors to influence Viterbi\\).", "title": "Implement virtual evidence during training", "destination": {"commit": {"hash": "be63c5ecae34", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/be63c5ecae34.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/be63c5ecae34"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "reason": "", "source": {"commit": {"hash": "e6dc8a0adad4", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway/commit/e6dc8a0adad4"}, "html": {"href": "https://bitbucket.org/rcwchan/segway/commits/e6dc8a0adad4"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway"}, "html": {"href": "https://bitbucket.org/rcwchan/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}ts=python"}}, "type": "repository", "name": "segway", "full_name": "rcwchan/segway", "uuid": "{285886c8-7a4a-4d80-8fe1-3d4b0d9146f6}"}, "branch": {"name": "virtualevidence"}}, "state": "OPEN", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "date": "2019-04-04T15:48:33.052183+00:00"}, "pull_request": {"type": "pullrequest", "id": 98, "links": {"self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/98.json"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/98"}}, "title": "Implement virtual evidence during training"}}]}