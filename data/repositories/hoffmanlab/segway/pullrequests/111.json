{"rendered": {"description": {"raw": "#### Introduction\r\n\r\nThis pull request modifies Segway\u2019s scheduling to allow for jobs to be submitted in \u2018batches\u2019 \\(defined hereon as \u2018a large script containing the segway-task commands of one or more smaller jobs\u2019\\) in a minimally invasive way for training jobs \\(non-bundle\\).\r\n\r\n#### Why\r\n\r\nDepending on the data, Segway\u2019s EM train jobs can often run very quickly \\(< 10 seconds\\). In these cases, a lot of the walltime is overhead from scheduling, which is highly undesirable. In addition, cluster systems often have strict rules about running many small jobs that would prevent some users from running Segway. The addition of an option for \u2018batch\u2019 scheduling should hopefully eliminate many of these issues.\r\n\r\n#### How to use\r\n\r\nTo enable this behavior, the user must specify `--jobs-per-batch` to be greater than 1. If `--jobs-per-batch=1` then we recover default, unbatched behavior \\(each segway-wrapper command \\(aka 'subjob'\\) is its own cluster job/script\\).\r\n\r\n#### Code changes\r\n\r\nThe primary changes are in `queue_train_parallel`. Here, I compute the expected number of batches \\(given `--jobs-per-batch`\\) and distribute the train windows in the current round into batches of maximum size `--jobs-per-batch`. In each batch, each subjob writes its segway-task command to a separate job script \\(as usual\\). After we write the job script of the final subjob in the batch, I accumulate all the subjob scripts into one large batch script file at once, after which the batch script is submitted as a job as usual.\r\n\r\n#### Possible issues\r\n\r\nThings that would be good to sanity check in case I\u2019ve made a mistake somewhere:\r\n\r\n* Since `queue_task` is reused by other functions \\(such as viterbi\\), we should be careful it retains the old behavior for these other functions.\r\n* I\u2019ve checked that the logic for the unbatched case is effectively the same as previous behavior, but this should be verified by someone else as well.\r\n\r\nSome things I don\u2019t know how to fix:\r\n\r\n* There doesn't seem to be a good way to check if the current job is a bundle job without checking if `subjob_index=None`.\r\n* My method of checking if we expect batched or unbatched behavior is not super elegant \\(checking if jobs per batch!=1, subjob\\_index is not None, etc\\).\r\n\r\n@{557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee} I have tried this PR locally using simpleseg \\(with various split-seq and jobs-per-batch options specified\\). After a first pass/sanity check, I will try it on the cluster. Thanks!", "markup": "markdown", "html": "<h4 id=\"markdown-header-introduction\">Introduction</h4>\n<p>This pull request modifies Segway\u2019s scheduling to allow for jobs to be submitted in \u2018batches\u2019 (defined hereon as \u2018a large script containing the segway-task commands of one or more smaller jobs\u2019) in a minimally invasive way for training jobs (non-bundle).</p>\n<h4 id=\"markdown-header-why\">Why</h4>\n<p>Depending on the data, Segway\u2019s EM train jobs can often run very quickly (&lt; 10 seconds). In these cases, a lot of the walltime is overhead from scheduling, which is highly undesirable. In addition, cluster systems often have strict rules about running many small jobs that would prevent some users from running Segway. The addition of an option for \u2018batch\u2019 scheduling should hopefully eliminate many of these issues.</p>\n<h4 id=\"markdown-header-how-to-use\">How to use</h4>\n<p>To enable this behavior, the user must specify <code>--jobs-per-batch</code> to be greater than 1. If <code>--jobs-per-batch=1</code> then we recover default, unbatched behavior (each segway-wrapper command (aka 'subjob') is its own cluster job/script).</p>\n<h4 id=\"markdown-header-code-changes\">Code changes</h4>\n<p>The primary changes are in <code>queue_train_parallel</code>. Here, I compute the expected number of batches (given <code>--jobs-per-batch</code>) and distribute the train windows in the current round into batches of maximum size <code>--jobs-per-batch</code>. In each batch, each subjob writes its segway-task command to a separate job script (as usual). After we write the job script of the final subjob in the batch, I accumulate all the subjob scripts into one large batch script file at once, after which the batch script is submitted as a job as usual.</p>\n<h4 id=\"markdown-header-possible-issues\">Possible issues</h4>\n<p>Things that would be good to sanity check in case I\u2019ve made a mistake somewhere:</p>\n<ul>\n<li>Since <code>queue_task</code> is reused by other functions (such as viterbi), we should be careful it retains the old behavior for these other functions.</li>\n<li>I\u2019ve checked that the logic for the unbatched case is effectively the same as previous behavior, but this should be verified by someone else as well.</li>\n</ul>\n<p>Some things I don\u2019t know how to fix:</p>\n<ul>\n<li>There doesn't seem to be a good way to check if the current job is a bundle job without checking if <code>subjob_index=None</code>.</li>\n<li>My method of checking if we expect batched or unbatched behavior is not super elegant (checking if jobs per batch!=1, subjob_index is not None, etc).</li>\n</ul>\n<p><span class=\"ap-mention ap-mention-me\" data-atlassian-id=\"557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee\">@Eric Roberts</span> I have tried this PR locally using simpleseg (with various split-seq and jobs-per-batch options specified). After a first pass/sanity check, I will try it on the cluster. Thanks!</p>", "type": "rendered"}, "title": {"raw": "Modify Segway's scheduling to allow for 'batch' job submission in training", "markup": "markdown", "html": "<p>Modify Segway's scheduling to allow for 'batch' job submission in training</p>", "type": "rendered"}}, "type": "pullrequest", "description": "#### Introduction\r\n\r\nThis pull request modifies Segway\u2019s scheduling to allow for jobs to be submitted in \u2018batches\u2019 \\(defined hereon as \u2018a large script containing the segway-task commands of one or more smaller jobs\u2019\\) in a minimally invasive way for training jobs \\(non-bundle\\).\r\n\r\n#### Why\r\n\r\nDepending on the data, Segway\u2019s EM train jobs can often run very quickly \\(< 10 seconds\\). In these cases, a lot of the walltime is overhead from scheduling, which is highly undesirable. In addition, cluster systems often have strict rules about running many small jobs that would prevent some users from running Segway. The addition of an option for \u2018batch\u2019 scheduling should hopefully eliminate many of these issues.\r\n\r\n#### How to use\r\n\r\nTo enable this behavior, the user must specify `--jobs-per-batch` to be greater than 1. If `--jobs-per-batch=1` then we recover default, unbatched behavior \\(each segway-wrapper command \\(aka 'subjob'\\) is its own cluster job/script\\).\r\n\r\n#### Code changes\r\n\r\nThe primary changes are in `queue_train_parallel`. Here, I compute the expected number of batches \\(given `--jobs-per-batch`\\) and distribute the train windows in the current round into batches of maximum size `--jobs-per-batch`. In each batch, each subjob writes its segway-task command to a separate job script \\(as usual\\). After we write the job script of the final subjob in the batch, I accumulate all the subjob scripts into one large batch script file at once, after which the batch script is submitted as a job as usual.\r\n\r\n#### Possible issues\r\n\r\nThings that would be good to sanity check in case I\u2019ve made a mistake somewhere:\r\n\r\n* Since `queue_task` is reused by other functions \\(such as viterbi\\), we should be careful it retains the old behavior for these other functions.\r\n* I\u2019ve checked that the logic for the unbatched case is effectively the same as previous behavior, but this should be verified by someone else as well.\r\n\r\nSome things I don\u2019t know how to fix:\r\n\r\n* There doesn't seem to be a good way to check if the current job is a bundle job without checking if `subjob_index=None`.\r\n* My method of checking if we expect batched or unbatched behavior is not super elegant \\(checking if jobs per batch!=1, subjob\\_index is not None, etc\\).\r\n\r\n@{557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee} I have tried this PR locally using simpleseg \\(with various split-seq and jobs-per-batch options specified\\). After a first pass/sanity check, I will try it on the cluster. Thanks!", "links": {"decline": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/pullrequests/111/decline"}, "diffstat": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/diffstat/rcwchan/segway-1:5c3ba8021f9e%0Df04cce0da8be?from_pullrequest_id=111"}, "commits": {"href": "data/repositories/hoffmanlab/segway/pullrequests/111/commits.json"}, "self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/111.json"}, "comments": {"href": "data/repositories/hoffmanlab/segway/pullrequests/111/comments_page=1.json"}, "merge": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/pullrequests/111/merge"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/111"}, "activity": {"href": "data/repositories/hoffmanlab/segway/pullrequests/111/activity.json"}, "diff": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/diff/rcwchan/segway-1:5c3ba8021f9e%0Df04cce0da8be?from_pullrequest_id=111"}, "approve": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/pullrequests/111/approve"}, "statuses": {"href": "data/repositories/hoffmanlab/segway/pullrequests/111/statuses_page=1.json"}}, "title": "Modify Segway's scheduling to allow for 'batch' job submission in training", "close_source_branch": false, "reviewers": [{"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}], "id": 111, "destination": {"commit": {"hash": "f04cce0da8be", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/f04cce0da8be.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/f04cce0da8be"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "created_on": "2019-08-21T14:41:40.650965+00:00", "summary": {"raw": "#### Introduction\r\n\r\nThis pull request modifies Segway\u2019s scheduling to allow for jobs to be submitted in \u2018batches\u2019 \\(defined hereon as \u2018a large script containing the segway-task commands of one or more smaller jobs\u2019\\) in a minimally invasive way for training jobs \\(non-bundle\\).\r\n\r\n#### Why\r\n\r\nDepending on the data, Segway\u2019s EM train jobs can often run very quickly \\(< 10 seconds\\). In these cases, a lot of the walltime is overhead from scheduling, which is highly undesirable. In addition, cluster systems often have strict rules about running many small jobs that would prevent some users from running Segway. The addition of an option for \u2018batch\u2019 scheduling should hopefully eliminate many of these issues.\r\n\r\n#### How to use\r\n\r\nTo enable this behavior, the user must specify `--jobs-per-batch` to be greater than 1. If `--jobs-per-batch=1` then we recover default, unbatched behavior \\(each segway-wrapper command \\(aka 'subjob'\\) is its own cluster job/script\\).\r\n\r\n#### Code changes\r\n\r\nThe primary changes are in `queue_train_parallel`. Here, I compute the expected number of batches \\(given `--jobs-per-batch`\\) and distribute the train windows in the current round into batches of maximum size `--jobs-per-batch`. In each batch, each subjob writes its segway-task command to a separate job script \\(as usual\\). After we write the job script of the final subjob in the batch, I accumulate all the subjob scripts into one large batch script file at once, after which the batch script is submitted as a job as usual.\r\n\r\n#### Possible issues\r\n\r\nThings that would be good to sanity check in case I\u2019ve made a mistake somewhere:\r\n\r\n* Since `queue_task` is reused by other functions \\(such as viterbi\\), we should be careful it retains the old behavior for these other functions.\r\n* I\u2019ve checked that the logic for the unbatched case is effectively the same as previous behavior, but this should be verified by someone else as well.\r\n\r\nSome things I don\u2019t know how to fix:\r\n\r\n* There doesn't seem to be a good way to check if the current job is a bundle job without checking if `subjob_index=None`.\r\n* My method of checking if we expect batched or unbatched behavior is not super elegant \\(checking if jobs per batch!=1, subjob\\_index is not None, etc\\).\r\n\r\n@{557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee} I have tried this PR locally using simpleseg \\(with various split-seq and jobs-per-batch options specified\\). After a first pass/sanity check, I will try it on the cluster. Thanks!", "markup": "markdown", "html": "<h4 id=\"markdown-header-introduction\">Introduction</h4>\n<p>This pull request modifies Segway\u2019s scheduling to allow for jobs to be submitted in \u2018batches\u2019 (defined hereon as \u2018a large script containing the segway-task commands of one or more smaller jobs\u2019) in a minimally invasive way for training jobs (non-bundle).</p>\n<h4 id=\"markdown-header-why\">Why</h4>\n<p>Depending on the data, Segway\u2019s EM train jobs can often run very quickly (&lt; 10 seconds). In these cases, a lot of the walltime is overhead from scheduling, which is highly undesirable. In addition, cluster systems often have strict rules about running many small jobs that would prevent some users from running Segway. The addition of an option for \u2018batch\u2019 scheduling should hopefully eliminate many of these issues.</p>\n<h4 id=\"markdown-header-how-to-use\">How to use</h4>\n<p>To enable this behavior, the user must specify <code>--jobs-per-batch</code> to be greater than 1. If <code>--jobs-per-batch=1</code> then we recover default, unbatched behavior (each segway-wrapper command (aka 'subjob') is its own cluster job/script).</p>\n<h4 id=\"markdown-header-code-changes\">Code changes</h4>\n<p>The primary changes are in <code>queue_train_parallel</code>. Here, I compute the expected number of batches (given <code>--jobs-per-batch</code>) and distribute the train windows in the current round into batches of maximum size <code>--jobs-per-batch</code>. In each batch, each subjob writes its segway-task command to a separate job script (as usual). After we write the job script of the final subjob in the batch, I accumulate all the subjob scripts into one large batch script file at once, after which the batch script is submitted as a job as usual.</p>\n<h4 id=\"markdown-header-possible-issues\">Possible issues</h4>\n<p>Things that would be good to sanity check in case I\u2019ve made a mistake somewhere:</p>\n<ul>\n<li>Since <code>queue_task</code> is reused by other functions (such as viterbi), we should be careful it retains the old behavior for these other functions.</li>\n<li>I\u2019ve checked that the logic for the unbatched case is effectively the same as previous behavior, but this should be verified by someone else as well.</li>\n</ul>\n<p>Some things I don\u2019t know how to fix:</p>\n<ul>\n<li>There doesn't seem to be a good way to check if the current job is a bundle job without checking if <code>subjob_index=None</code>.</li>\n<li>My method of checking if we expect batched or unbatched behavior is not super elegant (checking if jobs per batch!=1, subjob_index is not None, etc).</li>\n</ul>\n<p><span class=\"ap-mention ap-mention-me\" data-atlassian-id=\"557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee\">@Eric Roberts</span> I have tried this PR locally using simpleseg (with various split-seq and jobs-per-batch options specified). After a first pass/sanity check, I will try it on the cluster. Thanks!</p>", "type": "rendered"}, "source": {"commit": {"hash": "5c3ba8021f9e", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway-1/commit/5c3ba8021f9e"}, "html": {"href": "https://bitbucket.org/rcwchan/segway-1/commits/5c3ba8021f9e"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/rcwchan/segway-1"}, "html": {"href": "https://bitbucket.org/rcwchan/segway-1"}, "avatar": {"href": "data/bytebucket.org/ravatar/{cfe37561-5057-4cfe-911c-557786e4580b}ts=python"}}, "type": "repository", "name": "segway-1", "full_name": "rcwchan/segway-1", "uuid": "{cfe37561-5057-4cfe-911c-557786e4580b}"}, "branch": {"name": "batch_job_submission"}}, "comment_count": 9, "state": "OPEN", "task_count": 0, "participants": [{"role": "REVIEWER", "participated_on": "2019-08-26T16:14:04.060548+00:00", "type": "participant", "approved": false, "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}}, {"role": "PARTICIPANT", "participated_on": "2019-09-13T20:16:13.921895+00:00", "type": "participant", "approved": false, "user": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}}], "reason": "", "updated_on": "2019-09-13T20:16:13.921895+00:00", "author": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "merge_commit": null, "closed_by": null}