{"rendered": {"description": {"raw": "## Introduction\r\n\r\nThis pull request implements virtual evidence for all available segway tasks, building off of existing work from both Rachel Chan and Max Libbrecht\u2019s implementations of VE for training.\r\n\r\nDescription provided by Rachel Chan in her original [pull request](#!/hoffmanlab/segway/pull-requests/98) is as follows:  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence \\(as there is a different Ct|Qt CPT at each t.\\)\r\n\r\n![](data/bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png)\r\n\u200c\r\n\r\n## User Interface\r\n\r\nNo change for users not running Virtual Evidence.\r\n\r\nUsers supplying virtual evidence while running an entire task together \\(ie segway train or segway annotate\\) will supply the virtual evidence file with --virtual-evidence.\r\n\r\nUsers supplying virtual evidence while running steps for a task one at a time will need to specify --virtual-evidence during the init step in order to properly generate the input files such as input.master and the tri file. The tri file will be generated from GMTK without the virtual evidence specification, and will therefore be missing virtual evidence tracks unless it is specified during init. This can be a placeholder, though a warning will be shown saying the file does not exist. Whatever is supplied by --virtual-evidence during train-init will then be passed to train-run. This file can however be overridden by using the --virtual-evidence option again for train-run or train-run-round.\r\n\r\nDuring annotate and posterior a file can be supplied with the --virtual-evidence option to the init step however it may not be changed during run.\r\n\r\nIn addition to support for virtual-evidence with the more modular API, two new model variables have been added, --track-weight and --virtual-evidence-weight. The track weight applies an exponent to all track data provided to the model to skew it away from uniform, while virtual evidence weight does the same thing for virtual evidence data specifically.\r\n\r\n## Input File\r\n\r\nThe virtual evidence file should be of BED3\\+2 format where the 4th column is the label index and the fifth column is the prior. For example,\r\n\r\n`chr1 0 1000 0 0.9`\r\n\r\nIf running on multiple worlds, the VE file is in BED3\\+3 format instead, and the world number must be specified for each row in the last column. If this is omitted and a BED3\\+2 file is submitted instead, the virtual evidence will be applied to all worlds instead. Using the previous example, in the second world,\r\n\r\n`chr1 0 1000 0 0.9 1`\r\n\r\nThese examples specify a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n`chr1 0 1000 0 0.4`\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\n## Resolution and downsampling\r\n\r\nSection taken from Rachel Chan\u2019s original PR, with very slight modification for new interface:\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n`chr1 0 1 0 0.5`\r\n\r\n`chr1 0 1 1 0.2`\r\n\r\n`chr1 1 2 1 0.4`\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n`label 0 1 2`\r\n\r\n`chr1 0 1 0.5 0.2 0.3`\r\n\r\n`chr1 1 2 0.3 0.4 0.3`\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n`1 chr1 0 10 0.4 0.3 0.4`\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nThis is a test case created and described by Rachel Chan. We supply the following virtual evidence priors to the simpleseg test case:\r\n\r\n`chr1 0 1000 0 0.9`\r\n\r\n`chr1 4000 5000 0 0.9`\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n`chr1 0 1000 0 1000 . 0 1000 27,158,119`\r\n\r\n`chr1 1000 2000 1 1000 . 1000 2000 217,95,2`\r\n\r\n`chr1 2000 3000 2 1000 . 2000 3000 117,112,179`\r\n\r\n`chr1 3000 4000 3 1000 . 3000 4000 231,41,138`\r\n\r\n`chr1 4000 5000 0 1000 . 4000 5000 27,158,119`\r\n\r\n`chr1 5000 6000 1 1000 . 5000 6000 217,95,2`\r\n\r\n`chr1 6000 7000 2 1000 . 6000 7000 117,112,179`\r\n\r\n`chr1 7000 8000 3 1000 . 7000 8000 231,41,138`\r\n\r\nThis test case has been named `simplevirtualevidence`.", "markup": "markdown", "html": "<h2 id=\"markdown-header-introduction\">Introduction</h2>\n<p>This pull request implements virtual evidence for all available segway tasks, building off of existing work from both Rachel Chan and Max Libbrecht\u2019s implementations of VE for training.</p>\n<p>Description provided by Rachel Chan in her original <a data-is-external-link=\"true\" href=\"#!/hoffmanlab/segway/pull-requests/98\" rel=\"nofollow\">pull request</a> is as follows:<br />\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent (segment label) A at a particular position is a particular label (ie P(C=1|A=a)).</p>\n<p>In other words, the binary virtual child C exists only to constrain the parent (segment label) to be a particular value (supervised label) with the specified prior probability.</p>\n<p>I have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.</p>\n<p>The revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence (as there is a different Ct|Qt CPT at each t.)</p>\n<p><img alt=\"\" src=\"data/bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png\" />\n\u200c</p>\n<h2 id=\"markdown-header-user-interface\">User Interface</h2>\n<p>No change for users not running Virtual Evidence.</p>\n<p>Users supplying virtual evidence while running an entire task together (ie segway train or segway annotate) will supply the virtual evidence file with --virtual-evidence.</p>\n<p>Users supplying virtual evidence while running steps for a task one at a time will need to specify --virtual-evidence during the init step in order to properly generate the input files such as input.master and the tri file. The tri file will be generated from GMTK without the virtual evidence specification, and will therefore be missing virtual evidence tracks unless it is specified during init. This can be a placeholder, though a warning will be shown saying the file does not exist. Whatever is supplied by --virtual-evidence during train-init will then be passed to train-run. This file can however be overridden by using the --virtual-evidence option again for train-run or train-run-round.</p>\n<p>During annotate and posterior a file can be supplied with the --virtual-evidence option to the init step however it may not be changed during run.</p>\n<p>In addition to support for virtual-evidence with the more modular API, two new model variables have been added, --track-weight and --virtual-evidence-weight. The track weight applies an exponent to all track data provided to the model to skew it away from uniform, while virtual evidence weight does the same thing for virtual evidence data specifically.</p>\n<h2 id=\"markdown-header-input-file\">Input File</h2>\n<p>The virtual evidence file should be of BED3+2 format where the 4th column is the label index and the fifth column is the prior. For example,</p>\n<p><code>chr1 0 1000 0 0.9</code></p>\n<p>If running on multiple worlds, the VE file is in BED3+3 format instead, and the world number must be specified for each row in the last column. If this is omitted and a BED3+2 file is submitted instead, the virtual evidence will be applied to all worlds instead. Using the previous example, in the second world,</p>\n<p><code>chr1 0 1000 0 0.9 1</code></p>\n<p>These examples specify a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1(0,1000).</p>\n<p>At positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:</p>\n<p><code>chr1 0 1000 0 0.4</code></p>\n<p>all labels but label 0 would be given a prior probability of (1-0.4)/3=0.2.</p>\n<h2 id=\"markdown-header-resolution-and-downsampling\">Resolution and downsampling</h2>\n<p>Section taken from Rachel Chan\u2019s original PR, with very slight modification for new interface:</p>\n<p>In order to downsample virtual evidence (<code>get_downsampled_virtual_evidence_data_and_presence</code>), I decided to implement the following rule:</p>\n<p>Let resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability (over the position axis) over these positions. The presence is then the number of positions that have any priors specified.</p>\n<p>For example, with 3 total labels with resolution 10:</p>\n<p><code>chr1 0 1 0 0.5</code></p>\n<p><code>chr1 0 1 1 0.2</code></p>\n<p><code>chr1 1 2 1 0.4</code></p>\n<p>this expands (inferring uniform priors for remaining labels) into:</p>\n<p><code>label 0 1 2</code></p>\n<p><code>chr1 0 1 0.5 0.2 0.3</code></p>\n<p><code>chr1 1 2 0.3 0.4 0.3</code></p>\n<p>we take the average over these positions to obtain:</p>\n<p><code>1 chr1 0 10 0.4 0.3 0.4</code></p>\n<p>with presence 2.</p>\n<p>Now let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.</p>\n<p>In both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.</p>\n<h2 id=\"markdown-header-test-case\">Test case</h2>\n<p>This is a test case created and described by Rachel Chan. We supply the following virtual evidence priors to the simpleseg test case:</p>\n<p><code>chr1 0 1000 0 0.9</code></p>\n<p><code>chr1 4000 5000 0 0.9</code></p>\n<p>This is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:</p>\n<p><code>chr1 0 1000 0 1000 . 0 1000 27,158,119</code></p>\n<p><code>chr1 1000 2000 1 1000 . 1000 2000 217,95,2</code></p>\n<p><code>chr1 2000 3000 2 1000 . 2000 3000 117,112,179</code></p>\n<p><code>chr1 3000 4000 3 1000 . 3000 4000 231,41,138</code></p>\n<p><code>chr1 4000 5000 0 1000 . 4000 5000 27,158,119</code></p>\n<p><code>chr1 5000 6000 1 1000 . 5000 6000 217,95,2</code></p>\n<p><code>chr1 6000 7000 2 1000 . 6000 7000 117,112,179</code></p>\n<p><code>chr1 7000 8000 3 1000 . 7000 8000 231,41,138</code></p>\n<p>This test case has been named <code>simplevirtualevidence</code>.</p>", "type": "rendered"}, "title": {"raw": "Implement virtual evidence for all segway tasks", "markup": "markdown", "html": "<p>Implement virtual evidence for all segway tasks</p>", "type": "rendered"}}, "type": "pullrequest", "description": "## Introduction\r\n\r\nThis pull request implements virtual evidence for all available segway tasks, building off of existing work from both Rachel Chan and Max Libbrecht\u2019s implementations of VE for training.\r\n\r\nDescription provided by Rachel Chan in her original [pull request](#!/hoffmanlab/segway/pull-requests/98) is as follows:  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence \\(as there is a different Ct|Qt CPT at each t.\\)\r\n\r\n![](data/bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png)\r\n\u200c\r\n\r\n## User Interface\r\n\r\nNo change for users not running Virtual Evidence.\r\n\r\nUsers supplying virtual evidence while running an entire task together \\(ie segway train or segway annotate\\) will supply the virtual evidence file with --virtual-evidence.\r\n\r\nUsers supplying virtual evidence while running steps for a task one at a time will need to specify --virtual-evidence during the init step in order to properly generate the input files such as input.master and the tri file. The tri file will be generated from GMTK without the virtual evidence specification, and will therefore be missing virtual evidence tracks unless it is specified during init. This can be a placeholder, though a warning will be shown saying the file does not exist. Whatever is supplied by --virtual-evidence during train-init will then be passed to train-run. This file can however be overridden by using the --virtual-evidence option again for train-run or train-run-round.\r\n\r\nDuring annotate and posterior a file can be supplied with the --virtual-evidence option to the init step however it may not be changed during run.\r\n\r\nIn addition to support for virtual-evidence with the more modular API, two new model variables have been added, --track-weight and --virtual-evidence-weight. The track weight applies an exponent to all track data provided to the model to skew it away from uniform, while virtual evidence weight does the same thing for virtual evidence data specifically.\r\n\r\n## Input File\r\n\r\nThe virtual evidence file should be of BED3\\+2 format where the 4th column is the label index and the fifth column is the prior. For example,\r\n\r\n`chr1 0 1000 0 0.9`\r\n\r\nIf running on multiple worlds, the VE file is in BED3\\+3 format instead, and the world number must be specified for each row in the last column. If this is omitted and a BED3\\+2 file is submitted instead, the virtual evidence will be applied to all worlds instead. Using the previous example, in the second world,\r\n\r\n`chr1 0 1000 0 0.9 1`\r\n\r\nThese examples specify a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n`chr1 0 1000 0 0.4`\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\n## Resolution and downsampling\r\n\r\nSection taken from Rachel Chan\u2019s original PR, with very slight modification for new interface:\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n`chr1 0 1 0 0.5`\r\n\r\n`chr1 0 1 1 0.2`\r\n\r\n`chr1 1 2 1 0.4`\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n`label 0 1 2`\r\n\r\n`chr1 0 1 0.5 0.2 0.3`\r\n\r\n`chr1 1 2 0.3 0.4 0.3`\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n`1 chr1 0 10 0.4 0.3 0.4`\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nThis is a test case created and described by Rachel Chan. We supply the following virtual evidence priors to the simpleseg test case:\r\n\r\n`chr1 0 1000 0 0.9`\r\n\r\n`chr1 4000 5000 0 0.9`\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n`chr1 0 1000 0 1000 . 0 1000 27,158,119`\r\n\r\n`chr1 1000 2000 1 1000 . 1000 2000 217,95,2`\r\n\r\n`chr1 2000 3000 2 1000 . 2000 3000 117,112,179`\r\n\r\n`chr1 3000 4000 3 1000 . 3000 4000 231,41,138`\r\n\r\n`chr1 4000 5000 0 1000 . 4000 5000 27,158,119`\r\n\r\n`chr1 5000 6000 1 1000 . 5000 6000 217,95,2`\r\n\r\n`chr1 6000 7000 2 1000 . 6000 7000 117,112,179`\r\n\r\n`chr1 7000 8000 3 1000 . 7000 8000 231,41,138`\r\n\r\nThis test case has been named `simplevirtualevidence`.", "links": {"decline": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/pullrequests/100/decline"}, "diffstat": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/diffstat/hoffmanlab/segway:05ab48931538%0D8cbdd3f79d65?from_pullrequest_id=100"}, "commits": {"href": "data/repositories/hoffmanlab/segway/pullrequests/100/commits.json"}, "self": {"href": "data/repositories/hoffmanlab/segway/pullrequests/100.json"}, "comments": {"href": "data/repositories/hoffmanlab/segway/pullrequests/100/comments_page=1.json"}, "merge": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/pullrequests/100/merge"}, "html": {"href": "#!/hoffmanlab/segway/pull-requests/100"}, "activity": {"href": "data/repositories/hoffmanlab/segway/pullrequests/100/activity.json"}, "diff": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/diff/hoffmanlab/segway:05ab48931538%0D8cbdd3f79d65?from_pullrequest_id=100"}, "approve": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/pullrequests/100/approve"}, "statuses": {"href": "data/repositories/hoffmanlab/segway/pullrequests/100/statuses_page=1.json"}}, "title": "Implement virtual evidence for all segway tasks", "close_source_branch": false, "reviewers": [{"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}], "id": 100, "destination": {"commit": {"hash": "8cbdd3f79d65", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/8cbdd3f79d65.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/8cbdd3f79d65"}}}, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "branch": {"name": "default"}}, "created_on": "2019-05-24T14:11:09.122989+00:00", "summary": {"raw": "## Introduction\r\n\r\nThis pull request implements virtual evidence for all available segway tasks, building off of existing work from both Rachel Chan and Max Libbrecht\u2019s implementations of VE for training.\r\n\r\nDescription provided by Rachel Chan in her original [pull request](#!/hoffmanlab/segway/pull-requests/98) is as follows:  \r\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent \\(segment label\\) A at a particular position is a particular label \\(ie P\\(C=1|A=a\\)\\).\r\n\r\nIn other words, the binary virtual child C exists only to constrain the parent \\(segment label\\) to be a particular value \\(supervised label\\) with the specified prior probability.\r\n\r\nI have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.\r\n\r\nThe revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence \\(as there is a different Ct|Qt CPT at each t.\\)\r\n\r\n![](data/bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png)\r\n\u200c\r\n\r\n## User Interface\r\n\r\nNo change for users not running Virtual Evidence.\r\n\r\nUsers supplying virtual evidence while running an entire task together \\(ie segway train or segway annotate\\) will supply the virtual evidence file with --virtual-evidence.\r\n\r\nUsers supplying virtual evidence while running steps for a task one at a time will need to specify --virtual-evidence during the init step in order to properly generate the input files such as input.master and the tri file. The tri file will be generated from GMTK without the virtual evidence specification, and will therefore be missing virtual evidence tracks unless it is specified during init. This can be a placeholder, though a warning will be shown saying the file does not exist. Whatever is supplied by --virtual-evidence during train-init will then be passed to train-run. This file can however be overridden by using the --virtual-evidence option again for train-run or train-run-round.\r\n\r\nDuring annotate and posterior a file can be supplied with the --virtual-evidence option to the init step however it may not be changed during run.\r\n\r\nIn addition to support for virtual-evidence with the more modular API, two new model variables have been added, --track-weight and --virtual-evidence-weight. The track weight applies an exponent to all track data provided to the model to skew it away from uniform, while virtual evidence weight does the same thing for virtual evidence data specifically.\r\n\r\n## Input File\r\n\r\nThe virtual evidence file should be of BED3\\+2 format where the 4th column is the label index and the fifth column is the prior. For example,\r\n\r\n`chr1 0 1000 0 0.9`\r\n\r\nIf running on multiple worlds, the VE file is in BED3\\+3 format instead, and the world number must be specified for each row in the last column. If this is omitted and a BED3\\+2 file is submitted instead, the virtual evidence will be applied to all worlds instead. Using the previous example, in the second world,\r\n\r\n`chr1 0 1000 0 0.9 1`\r\n\r\nThese examples specify a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1\\(0,1000\\).\r\n\r\nAt positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:\r\n\r\n`chr1 0 1000 0 0.4`\r\n\r\nall labels but label 0 would be given a prior probability of \\(1-0.4\\)/3=0.2.\r\n\r\n## Resolution and downsampling\r\n\r\nSection taken from Rachel Chan\u2019s original PR, with very slight modification for new interface:\r\n\r\nIn order to downsample virtual evidence \\(`get_downsampled_virtual_evidence_data_and_presence`\\), I decided to implement the following rule:\r\n\r\nLet resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability \\(over the position axis\\) over these positions. The presence is then the number of positions that have any priors specified.\r\n\r\nFor example, with 3 total labels with resolution 10:\r\n\r\n`chr1 0 1 0 0.5`\r\n\r\n`chr1 0 1 1 0.2`\r\n\r\n`chr1 1 2 1 0.4`\r\n\r\nthis expands \\(inferring uniform priors for remaining labels\\) into:\r\n\r\n`label 0 1 2`\r\n\r\n`chr1 0 1 0.5 0.2 0.3`\r\n\r\n`chr1 1 2 0.3 0.4 0.3`\r\n\r\nwe take the average over these positions to obtain:\r\n\r\n`1 chr1 0 10 0.4 0.3 0.4`\r\n\r\nwith presence 2.\r\n\r\nNow let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.\r\n\r\nIn both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.\r\n\r\n## Test case\r\n\r\nThis is a test case created and described by Rachel Chan. We supply the following virtual evidence priors to the simpleseg test case:\r\n\r\n`chr1 0 1000 0 0.9`\r\n\r\n`chr1 4000 5000 0 0.9`\r\n\r\nThis is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:\r\n\r\n`chr1 0 1000 0 1000 . 0 1000 27,158,119`\r\n\r\n`chr1 1000 2000 1 1000 . 1000 2000 217,95,2`\r\n\r\n`chr1 2000 3000 2 1000 . 2000 3000 117,112,179`\r\n\r\n`chr1 3000 4000 3 1000 . 3000 4000 231,41,138`\r\n\r\n`chr1 4000 5000 0 1000 . 4000 5000 27,158,119`\r\n\r\n`chr1 5000 6000 1 1000 . 5000 6000 217,95,2`\r\n\r\n`chr1 6000 7000 2 1000 . 6000 7000 117,112,179`\r\n\r\n`chr1 7000 8000 3 1000 . 7000 8000 231,41,138`\r\n\r\nThis test case has been named `simplevirtualevidence`.", "markup": "markdown", "html": "<h2 id=\"markdown-header-introduction\">Introduction</h2>\n<p>This pull request implements virtual evidence for all available segway tasks, building off of existing work from both Rachel Chan and Max Libbrecht\u2019s implementations of VE for training.</p>\n<p>Description provided by Rachel Chan in her original <a data-is-external-link=\"true\" href=\"#!/hoffmanlab/segway/pull-requests/98\" rel=\"nofollow\">pull request</a> is as follows:<br />\nSpecifically, a binary virtual evidence node C has been added, which supplies the prior probability that the parent (segment label) A at a particular position is a particular label (ie P(C=1|A=a)).</p>\n<p>In other words, the binary virtual child C exists only to constrain the parent (segment label) to be a particular value (supervised label) with the specified prior probability.</p>\n<p>I have also added a presence node such that positions with no prior specified by the user will have no virtual evidence.</p>\n<p>The revised part of the graphical model is as below, where Qt is the segment label at position t, Ct is the virtual evidence child, and oCt indicates the presence of Ct. The black dotted line indicates a deterministic dependence where the parent of Ct switches based on the value of oCt; the red line indicates a stochastic conditional dependence (as there is a different Ct|Qt CPT at each t.)</p>\n<p><img alt=\"\" src=\"data/bitbucket.org/repo/a7MnLo/images/3913472725-graph_changes.png\" />\n\u200c</p>\n<h2 id=\"markdown-header-user-interface\">User Interface</h2>\n<p>No change for users not running Virtual Evidence.</p>\n<p>Users supplying virtual evidence while running an entire task together (ie segway train or segway annotate) will supply the virtual evidence file with --virtual-evidence.</p>\n<p>Users supplying virtual evidence while running steps for a task one at a time will need to specify --virtual-evidence during the init step in order to properly generate the input files such as input.master and the tri file. The tri file will be generated from GMTK without the virtual evidence specification, and will therefore be missing virtual evidence tracks unless it is specified during init. This can be a placeholder, though a warning will be shown saying the file does not exist. Whatever is supplied by --virtual-evidence during train-init will then be passed to train-run. This file can however be overridden by using the --virtual-evidence option again for train-run or train-run-round.</p>\n<p>During annotate and posterior a file can be supplied with the --virtual-evidence option to the init step however it may not be changed during run.</p>\n<p>In addition to support for virtual-evidence with the more modular API, two new model variables have been added, --track-weight and --virtual-evidence-weight. The track weight applies an exponent to all track data provided to the model to skew it away from uniform, while virtual evidence weight does the same thing for virtual evidence data specifically.</p>\n<h2 id=\"markdown-header-input-file\">Input File</h2>\n<p>The virtual evidence file should be of BED3+2 format where the 4th column is the label index and the fifth column is the prior. For example,</p>\n<p><code>chr1 0 1000 0 0.9</code></p>\n<p>If running on multiple worlds, the VE file is in BED3+3 format instead, and the world number must be specified for each row in the last column. If this is omitted and a BED3+2 file is submitted instead, the virtual evidence will be applied to all worlds instead. Using the previous example, in the second world,</p>\n<p><code>chr1 0 1000 0 0.9 1</code></p>\n<p>These examples specify a prior probability of 0.9 on label 0 and 0.05 on label 1 at chr1(0,1000).</p>\n<p>At positions for which some labels are given a prior by the user but other labels not, the remaining probability is uniformly distributed amongst the leftover labels. For example, with 4 labels:</p>\n<p><code>chr1 0 1000 0 0.4</code></p>\n<p>all labels but label 0 would be given a prior probability of (1-0.4)/3=0.2.</p>\n<h2 id=\"markdown-header-resolution-and-downsampling\">Resolution and downsampling</h2>\n<p>Section taken from Rachel Chan\u2019s original PR, with very slight modification for new interface:</p>\n<p>In order to downsample virtual evidence (<code>get_downsampled_virtual_evidence_data_and_presence</code>), I decided to implement the following rule:</p>\n<p>Let resolution be greater than 1. For positions which have some priors specified but not all, compute uniform priors for the remaining labels and take the average prior probability (over the position axis) over these positions. The presence is then the number of positions that have any priors specified.</p>\n<p>For example, with 3 total labels with resolution 10:</p>\n<p><code>chr1 0 1 0 0.5</code></p>\n<p><code>chr1 0 1 1 0.2</code></p>\n<p><code>chr1 1 2 1 0.4</code></p>\n<p>this expands (inferring uniform priors for remaining labels) into:</p>\n<p><code>label 0 1 2</code></p>\n<p><code>chr1 0 1 0.5 0.2 0.3</code></p>\n<p><code>chr1 1 2 0.3 0.4 0.3</code></p>\n<p>we take the average over these positions to obtain:</p>\n<p><code>1 chr1 0 10 0.4 0.3 0.4</code></p>\n<p>with presence 2.</p>\n<p>Now let resolution equal 1. Then the presence is simply 1 or 0; 1 for positions with priors specified, and 0 for positions without. For positions with priors specified, uniform priors are inferred for any remaining labels, and this becomes the prior vector for that position.</p>\n<p>In both cases, positions with no priors specified are given 0 presence and uniform priors for all labels.</p>\n<h2 id=\"markdown-header-test-case\">Test case</h2>\n<p>This is a test case created and described by Rachel Chan. We supply the following virtual evidence priors to the simpleseg test case:</p>\n<p><code>chr1 0 1000 0 0.9</code></p>\n<p><code>chr1 4000 5000 0 0.9</code></p>\n<p>This is able to separate the binary 0,0 and 0,1 regions in simpleseg, creating the following annotation:</p>\n<p><code>chr1 0 1000 0 1000 . 0 1000 27,158,119</code></p>\n<p><code>chr1 1000 2000 1 1000 . 1000 2000 217,95,2</code></p>\n<p><code>chr1 2000 3000 2 1000 . 2000 3000 117,112,179</code></p>\n<p><code>chr1 3000 4000 3 1000 . 3000 4000 231,41,138</code></p>\n<p><code>chr1 4000 5000 0 1000 . 4000 5000 27,158,119</code></p>\n<p><code>chr1 5000 6000 1 1000 . 5000 6000 217,95,2</code></p>\n<p><code>chr1 6000 7000 2 1000 . 6000 7000 117,112,179</code></p>\n<p><code>chr1 7000 8000 3 1000 . 7000 8000 231,41,138</code></p>\n<p>This test case has been named <code>simplevirtualevidence</code>.</p>", "type": "rendered"}, "source": {"commit": {"hash": "a1503e066cd1", "type": "commit", "links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/mjmm13/segway_python23/commit/a1503e066cd1"}, "html": {"href": "https://bitbucket.org/mjmm13/segway_python23/commits/a1503e066cd1"}}}, "repository": {"links": {"self": {"href": "https://api.bitbucket.org/2.0/repositories/mjmm13/segway_python23"}, "html": {"href": "https://bitbucket.org/mjmm13/segway_python23"}, "avatar": {"href": "data/bytebucket.org/ravatar/{735c8db4-9d46-48a5-95b3-1d01db953c08}ts=python"}}, "type": "repository", "name": "segway_python23", "full_name": "mjmm13/segway_python23", "uuid": "{735c8db4-9d46-48a5-95b3-1d01db953c08}"}, "branch": {"name": "ve"}}, "comment_count": 132, "state": "MERGED", "task_count": 0, "participants": [{"role": "PARTICIPANT", "participated_on": "2020-01-06T20:19:16.913047+00:00", "type": "participant", "approved": false, "user": {"display_name": "Matthew McNeil", "uuid": "{018cd21a-1658-4202-9b1e-99dabc3e14d0}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B018cd21a-1658-4202-9b1e-99dabc3e14d0%7D"}, "html": {"href": "https://bitbucket.org/%7B018cd21a-1658-4202-9b1e-99dabc3e14d0%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/d7e7604c308bd957d77e1762b5caeddcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsMM-5.png"}}, "nickname": "mjmm13", "type": "user", "account_id": "5ac790531b5dd37ea53a123d"}}, {"role": "PARTICIPANT", "participated_on": "2019-07-19T20:47:53.406013+00:00", "type": "participant", "approved": false, "user": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}}, {"role": "REVIEWER", "participated_on": "2020-01-23T19:29:56.461299+00:00", "type": "participant", "approved": true, "user": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}}, {"role": "PARTICIPANT", "participated_on": "2020-01-09T04:52:33.245625+00:00", "type": "participant", "approved": false, "user": {"display_name": "Michael Hoffman", "uuid": "{ffa8e039-5d4d-4f69-a4ba-ac25cbaf700b}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bffa8e039-5d4d-4f69-a4ba-ac25cbaf700b%7D"}, "html": {"href": "https://bitbucket.org/%7Bffa8e039-5d4d-4f69-a4ba-ac25cbaf700b%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/6a95e857a02504cbad5fe965c9d9e4bbd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsMH-1.png"}}, "nickname": "hoffman", "type": "user", "account_id": "557058:a9657985-692c-405c-995b-4e41cda7ba2b"}}], "reason": "", "updated_on": "2020-01-23T19:43:08.577302+00:00", "author": {"display_name": "Matthew McNeil", "uuid": "{018cd21a-1658-4202-9b1e-99dabc3e14d0}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B018cd21a-1658-4202-9b1e-99dabc3e14d0%7D"}, "html": {"href": "https://bitbucket.org/%7B018cd21a-1658-4202-9b1e-99dabc3e14d0%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/d7e7604c308bd957d77e1762b5caeddcd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsMM-5.png"}}, "nickname": "mjmm13", "type": "user", "account_id": "5ac790531b5dd37ea53a123d"}, "merge_commit": {"hash": "05ab48931538", "type": "commit", "links": {"self": {"href": "data/repositories/hoffmanlab/segway/commit/05ab48931538.json"}, "html": {"href": "#!/hoffmanlab/segway/commits/05ab48931538"}}}, "closed_by": {"display_name": "Eric Roberts", "uuid": "{cd8c1fe0-28ca-45fb-8ef9-48090b42bb80}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D"}, "html": {"href": "https://bitbucket.org/%7Bcd8c1fe0-28ca-45fb-8ef9-48090b42bb80%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/17548d71dfb29d015b880f48cfc01ca3d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsER-5.png"}}, "nickname": "ericr86", "type": "user", "account_id": "557058:c80ca578-03a1-4ac6-b3ee-50372a3fceee"}}