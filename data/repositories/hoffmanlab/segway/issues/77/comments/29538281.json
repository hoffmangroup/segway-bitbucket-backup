{"links": {"self": {"href": "data/repositories/hoffmanlab/segway/issues/77/comments/29538281.json"}, "html": {"href": "#!/hoffmanlab/segway/issues/77#comment-29538281"}}, "issue": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway/issues/77.json"}}, "type": "issue", "id": 77, "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "title": "GMTK jobs report \"bad interpreter: Text file busy\""}, "content": {"raw": "The issue is most likely caused by NFS 3's default asynchronous behavior:\n\n```\n#!html\n\n>  The only thing I can think of is the fact that we change the \n> permissions on the file and writing out the metadata doesn't actually \n> complete by the time the separate process starts. When digging through \n> NFS docs I found that by default this is asynchronous behaviour:\n> \n>  \" This default permits the server to reply to client requests as soon \n> as it has processed the request and handed it off to the local file \n> system, without waiting for the data to be written to stable storage.\"\n> \n>  We've tried to fsync the file after changing permissions and before \n> using it but we also found the following:\n> \n>  \"Finally, note that, for NFS version 3 protocol requests, a \n> subsequent commit request from the NFS client at file close time, or \n> at fsync() time, will force the server to write any previously \n> unwritten data/metadata to the disk, and the server will not reply to \n> the client until this has been completed, as long as sync behavior is \n> followed. If async is used, the commit is essentially a no-op, since \n> the server once again lies to the client, telling the client that the \n> data has been sent to stable storage\".\n> \n>  If our scripts are indeed being stored on NFS then I can see why we \n> can't really work around this issue.\n\n```\n\nmordor is indeed running on NFS 3. Apparently there are plans to eventually migrate to NFS 4.\n\nSome things to note are: we might not get the error when running on the full cluster because of the time required to queue a job (rather than rapid-fire on a single node). Possible solutions suggested include: write only a single 'template' shell script and fill in the template as necessary for jobs: ie, queue the template script + arguments to fill in. \n\nUsers running on local will not get this error. It is difficult to reproduce even on a single node.", "markup": "markdown", "html": "<p>The issue is most likely caused by NFS 3's default asynchronous behavior:</p>\n<div class=\"codehilite language-html\"><pre><span></span>&gt;  The only thing I can think of is the fact that we change the \n&gt; permissions on the file and writing out the metadata doesn&#39;t actually \n&gt; complete by the time the separate process starts. When digging through \n&gt; NFS docs I found that by default this is asynchronous behaviour:\n&gt; \n&gt;  &quot; This default permits the server to reply to client requests as soon \n&gt; as it has processed the request and handed it off to the local file \n&gt; system, without waiting for the data to be written to stable storage.&quot;\n&gt; \n&gt;  We&#39;ve tried to fsync the file after changing permissions and before \n&gt; using it but we also found the following:\n&gt; \n&gt;  &quot;Finally, note that, for NFS version 3 protocol requests, a \n&gt; subsequent commit request from the NFS client at file close time, or \n&gt; at fsync() time, will force the server to write any previously \n&gt; unwritten data/metadata to the disk, and the server will not reply to \n&gt; the client until this has been completed, as long as sync behavior is \n&gt; followed. If async is used, the commit is essentially a no-op, since \n&gt; the server once again lies to the client, telling the client that the \n&gt; data has been sent to stable storage&quot;.\n&gt; \n&gt;  If our scripts are indeed being stored on NFS then I can see why we \n&gt; can&#39;t really work around this issue.\n</pre></div>\n\n\n<p>mordor is indeed running on NFS 3. Apparently there are plans to eventually migrate to NFS 4.</p>\n<p>Some things to note are: we might not get the error when running on the full cluster because of the time required to queue a job (rather than rapid-fire on a single node). Possible solutions suggested include: write only a single 'template' shell script and fill in the template as necessary for jobs: ie, queue the template script + arguments to fill in. </p>\n<p>Users running on local will not get this error. It is difficult to reproduce even on a single node.</p>", "type": "rendered"}, "created_on": "2016-07-29T18:32:22.939334+00:00", "user": {"display_name": "Rachel Chan", "uuid": "{20430f8d-4e6b-48cc-a8d8-c64868bf7e79}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D"}, "html": {"href": "https://bitbucket.org/%7B20430f8d-4e6b-48cc-a8d8-c64868bf7e79%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/4397abec9f84e35f1f235b350984833dd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsRC-1.png"}}, "nickname": "rcwchan", "type": "user", "account_id": "557058:e439e22e-8cfc-4cf1-b090-030d33a0730e"}, "updated_on": null, "type": "issue_comment", "id": 29538281}