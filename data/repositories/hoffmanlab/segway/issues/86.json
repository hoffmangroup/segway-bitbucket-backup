{"priority": "minor", "kind": "bug", "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "links": {"attachments": {"href": "data/repositories/hoffmanlab/segway/issues/86/attachments_page=1.json"}, "self": {"href": "data/repositories/hoffmanlab/segway/issues/86.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/issues/86/watch"}, "comments": {"href": "data/repositories/hoffmanlab/segway/issues/86/comments_page=1.json"}, "html": {"href": "#!/hoffmanlab/segway/issues/86/segway-identify-crashes-on-pbs-torque"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/issues/86/vote"}}, "reporter": {"display_name": "Gabriel Pratt", "uuid": "{e86348ec-7a86-406d-9b71-07a70fd5ec82}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Be86348ec-7a86-406d-9b71-07a70fd5ec82%7D"}, "html": {"href": "https://bitbucket.org/%7Be86348ec-7a86-406d-9b71-07a70fd5ec82%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/39e3276428e18646fe72b9e7f4d56282d=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsGP-0.png"}}, "nickname": "gpratt", "type": "user", "account_id": "557058:53bf62be-4cb1-4655-bf9e-5af9ab2e5329"}, "title": "Segway Identify crashes on PBS/Torque cluster", "component": null, "votes": 0, "watches": 1, "content": {"raw": "Hello, I'm just filing this issue for tracking sake, don't worry about fixing it, I've already fixed it locally, we can discuss if its worth building out a fix in general.\r\n\r\nOn my PBS/Torque cluster memory managed by something called \"cpuset\". Which allocates memory to a job based on the number of processors requested for that job.  This creates a conflict when Segway tries to specify memory requirements with the -l mem=XXGB and -l vmem=XXXGB. Causing Segway Identify to crash.  \r\n\r\nThe crash occurs when segway tries to use too much memory, I'm not quite sure what too much means in this case, but this wasn't an issue with segway train as it wasn't as memory intensive.  \r\n\r\nI fixed the issue by removing all resource requests from segway/segway/cluster/common.py Specifically line 76 was changed to \r\n```\r\n#!python\r\n\r\nself.res_req = []\r\n```\r\nThis allowed cpuset to manage the memory without conflicts and segway executed successfully.  \r\n\r\nI'm guessing that those memory requirements are important for resource allocation in other clusters, the best way I can think of fixing this issue generally is just putting in a flag that removes resource requests on clusters that don't need them or can't support them.\r\n     ", "markup": "markdown", "html": "<p>Hello, I'm just filing this issue for tracking sake, don't worry about fixing it, I've already fixed it locally, we can discuss if its worth building out a fix in general.</p>\n<p>On my PBS/Torque cluster memory managed by something called \"cpuset\". Which allocates memory to a job based on the number of processors requested for that job.  This creates a conflict when Segway tries to specify memory requirements with the -l mem=XXGB and -l vmem=XXXGB. Causing Segway Identify to crash.  </p>\n<p>The crash occurs when segway tries to use too much memory, I'm not quite sure what too much means in this case, but this wasn't an issue with segway train as it wasn't as memory intensive.  </p>\n<p>I fixed the issue by removing all resource requests from segway/segway/cluster/common.py Specifically line 76 was changed to </p>\n<div class=\"codehilite language-python\"><pre><span></span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">res_req</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n</pre></div>\n\n\n<p>This allowed cpuset to manage the memory without conflicts and segway executed successfully.  </p>\n<p>I'm guessing that those memory requirements are important for resource allocation in other clusters, the best way I can think of fixing this issue generally is just putting in a flag that removes resource requests on clusters that don't need them or can't support them.</p>", "type": "rendered"}, "assignee": null, "state": "new", "version": null, "edited_on": null, "created_on": "2016-09-27T18:52:31.163721+00:00", "milestone": null, "updated_on": "2016-09-27T18:59:43.515257+00:00", "type": "issue", "id": 86}