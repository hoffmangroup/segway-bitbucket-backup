{"priority": "trivial", "kind": "enhancement", "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "links": {"attachments": {"href": "data/repositories/hoffmanlab/segway/issues/34/attachments_page=1.json"}, "self": {"href": "data/repositories/hoffmanlab/segway/issues/34.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/issues/34/watch"}, "comments": {"href": "data/repositories/hoffmanlab/segway/issues/34/comments_page=1.json"}, "html": {"href": "#!/hoffmanlab/segway/issues/34/explicitly-limit-the-number-of-current"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/issues/34/vote"}}, "reporter": {"display_name": "Sakura Tamaki", "uuid": "{2e49c073-0cbe-4333-96b1-a81988007690}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B2e49c073-0cbe-4333-96b1-a81988007690%7D"}, "html": {"href": "https://bitbucket.org/%7B2e49c073-0cbe-4333-96b1-a81988007690%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:8f562ae3-ccee-4990-9d3f-58dc4e5d4282/860719ec-a458-4597-a2c0-04bb410a3373/128"}}, "nickname": "Tamaki_Sakura", "type": "user", "account_id": "557058:8f562ae3-ccee-4990-9d3f-58dc4e5d4282"}, "title": "Explicitly limit the number of current running segway jobs.", "component": null, "votes": 0, "watches": 2, "content": {"raw": "Currently, Segway will directly submit job to the cluster system as long as all the prerequisite of the job has finished. This is a reasonable behaviour with the assumption that segway should take all the available resources on a cluster.\r\n\r\nHowever, on most cluster system, computational resources used by segway is also shared by other people, where others might want to run a single job that may take a significant portion of the cluster's resources. Since each individual segway jobs is small, it is very easy to become the case that the agile segway take over all the resources in the cluster for a considerable period, making the jobs of others always on hold and not have the enough resources to run.\r\n\r\nOthers ways to solve this problem without changing segway code include specifying a resource quota per user on some cluster system, which would not be agile enough as it is normally only open to cluster administrator. Force all job sent by segway to be not run instantaneously and use a job monitor script to start part of them manually to leave enough resource for reserved is also a common solution (and is currently used in our lab), but without polling the cluster status too frequently by the script (which should not be done because it will consume too much resources) it is very hard to make it efficient.My test shown it to be ~50% slower.\r\n\r\nSo, what I am hoping segway could do is that segway can explicitly limit the number of current jobs running by limit the amount of job it sent to the cluster. It will not directly send jobs to the cluster as long as the number of job already sent to the cluster minus the number of job finished is greater than a specific amount. I am also hoping that this quota is changeable throughout the period of segway runs, perhaps through a shell environment variable.", "markup": "markdown", "html": "<p>Currently, Segway will directly submit job to the cluster system as long as all the prerequisite of the job has finished. This is a reasonable behaviour with the assumption that segway should take all the available resources on a cluster.</p>\n<p>However, on most cluster system, computational resources used by segway is also shared by other people, where others might want to run a single job that may take a significant portion of the cluster's resources. Since each individual segway jobs is small, it is very easy to become the case that the agile segway take over all the resources in the cluster for a considerable period, making the jobs of others always on hold and not have the enough resources to run.</p>\n<p>Others ways to solve this problem without changing segway code include specifying a resource quota per user on some cluster system, which would not be agile enough as it is normally only open to cluster administrator. Force all job sent by segway to be not run instantaneously and use a job monitor script to start part of them manually to leave enough resource for reserved is also a common solution (and is currently used in our lab), but without polling the cluster status too frequently by the script (which should not be done because it will consume too much resources) it is very hard to make it efficient.My test shown it to be ~50% slower.</p>\n<p>So, what I am hoping segway could do is that segway can explicitly limit the number of current jobs running by limit the amount of job it sent to the cluster. It will not directly send jobs to the cluster as long as the number of job already sent to the cluster minus the number of job finished is greater than a specific amount. I am also hoping that this quota is changeable throughout the period of segway runs, perhaps through a shell environment variable.</p>", "type": "rendered"}, "assignee": null, "state": "new", "version": null, "edited_on": null, "created_on": "2015-06-16T15:39:59.999534+00:00", "milestone": null, "updated_on": "2015-06-16T23:28:49.610072+00:00", "type": "issue", "id": 34}