{"priority": "trivial", "kind": "enhancement", "repository": {"links": {"self": {"href": "data/repositories/hoffmanlab/segway.json"}, "html": {"href": "#!/hoffmanlab/segway"}, "avatar": {"href": "data/bytebucket.org/ravatar/{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}ts=python"}}, "type": "repository", "name": "segway", "full_name": "hoffmanlab/segway", "uuid": "{68bf38bf-f0c1-4c9c-909e-0ee4b2c26257}"}, "links": {"attachments": {"href": "data/repositories/hoffmanlab/segway/issues/42/attachments_page=1.json"}, "self": {"href": "data/repositories/hoffmanlab/segway/issues/42.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/issues/42/watch"}, "comments": {"href": "data/repositories/hoffmanlab/segway/issues/42/comments_page=1.json"}, "html": {"href": "#!/hoffmanlab/segway/issues/42/too-many-log-files-in-single-directory"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/hoffmanlab/segway/issues/42/vote"}}, "reporter": {"display_name": "Michael Hoffman", "uuid": "{ffa8e039-5d4d-4f69-a4ba-ac25cbaf700b}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bffa8e039-5d4d-4f69-a4ba-ac25cbaf700b%7D"}, "html": {"href": "https://bitbucket.org/%7Bffa8e039-5d4d-4f69-a4ba-ac25cbaf700b%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/6a95e857a02504cbad5fe965c9d9e4bbd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsMH-1.png"}}, "nickname": "hoffman", "type": "user", "account_id": "557058:a9657985-692c-405c-995b-4e41cda7ba2b"}, "title": "Too many log files in single directory", "component": null, "votes": 0, "watches": 1, "content": {"raw": "Running `segway identify` for the whole human genome with default limits gives you thousands of jobs. This means that for log files that are generated once per file, we will get thousands in one directory. This causes performance problems for some filesystems and utilities.\r\n\r\nWe should split into multiple subdirectories when there are more than 1000 files. Maybe with subdirectories named `0`, `1`, `2`, etc. I think it would be inconvenient to have this splitting when there are fewer than 1000 files. This would unfortunately introduce an inconsistent behavior depending on the number of jobs.", "markup": "markdown", "html": "<p>Running <code>segway identify</code> for the whole human genome with default limits gives you thousands of jobs. This means that for log files that are generated once per file, we will get thousands in one directory. This causes performance problems for some filesystems and utilities.</p>\n<p>We should split into multiple subdirectories when there are more than 1000 files. Maybe with subdirectories named <code>0</code>, <code>1</code>, <code>2</code>, etc. I think it would be inconvenient to have this splitting when there are fewer than 1000 files. This would unfortunately introduce an inconsistent behavior depending on the number of jobs.</p>", "type": "rendered"}, "assignee": null, "state": "new", "version": null, "edited_on": null, "created_on": "2015-08-19T00:02:52.270567+00:00", "milestone": null, "updated_on": "2015-08-19T00:02:52.270567+00:00", "type": "issue", "id": 42}